# Analysis Summary Report

**Generated:** 2025-10-21 22:45:14

## Input Data

- **Results File:** `section1_3_results_20251021_215515.pkl`
- **Section Type:** section1_3
- **Dimensionality:** 2D
- **Models Directory:** Not provided

## Generated Outputs

### 1. Comparative Metrics Analysis

Location: `01_comparative_metrics/`

This directory contains:
- **Comparison tables** (CSV) for each dataset showing final MSE, training times, etc.
- **Learning curves** showing train/test/dense MSE over epochs
- **Training time comparisons** across all model types
- **Performance heatmaps** showing final MSE across all datasets and models

Files generated:
- `dataset_<N>_comparison_table.csv` - Detailed metrics for dataset N
- `dataset_<N>_learning_curves_<metric>.png` - Learning curves for each metric
- `dataset_<N>_training_times.png` - Training time comparisons
- `all_datasets_heatmap_<metric>.png` - Overall performance heatmaps

### 2. Function Fitting Visualizations

Location: `02_function_fitting/`

This directory contains visualizations comparing neural network predictions with true functions:


For 2D functions:
- **Surface plots** showing true function vs NN prediction
- **Contour plots** for easier comparison
- **MSE calculations** displayed on each plot

Each visualization shows all model types (MLP, SIREN, KAN, KAN with pruning) side-by-side.

### 3. Heatmap Analysis (2D Only)

Location: `03_heatmap_analysis/`

Detailed heatmap analysis for 2D functions:
- **Comparison heatmaps** - Side-by-side views of true function, prediction, and error
- **Error analysis** - Absolute error, signed error, and relative error maps
- **Cross-section plots** - 1D slices at fixed x₁ and x₂ values
- **Error quantile maps** - Identifying high-error regions
- **Error statistics** - Quantitative breakdown by region

Files generated per function and model:
- `heatmap_<N>_<function>_<model>.png` - Detailed comparison heatmaps
- `cross_section_<N>_<function>.png` - Cross-section comparisons
- `error_quantile_<N>_<function>_<model>.png` - Error quantile analysis

## How to Use These Results

### Quick Start

1. **Start with the heatmaps** in `01_comparative_metrics/all_datasets_heatmap_test.png` to see overall model performance
2. **Review learning curves** to understand training dynamics
3. **Examine function fits** to see how well models approximate each function
4. **Check training times** if computational efficiency is important

### Detailed Analysis

For each dataset/function of interest:

1. Open the **comparison table CSV** to see exact numerical values
2. Look at **learning curves** to check for overfitting or convergence issues
3. Examine **function fitting plots** to visually assess approximation quality
4. Review **heatmap analysis** to identify problematic regions in the domain
5. Check **cross-sections** to understand behavior along specific dimensions
6. Use **error quantile maps** to find where models struggle most

### Key Metrics

- **Train MSE**: Error on training data (lower is better)
- **Test MSE**: Error on test data (lower is better, indicates generalization)
- **Dense MSE**: Dense sampling MSE (better indicator of true approximation quality)
- **Total Time**: Complete training time in seconds
- **Time per Epoch**: Average time per training epoch

## Model Comparison

The analysis compares four model types:

1. **MLP**: Traditional multilayer perceptron with various depths and activations
2. **SIREN**: Sinusoidal activation networks, specialized for periodic functions
3. **KAN**: Kolmogorov-Arnold Networks with various grid sizes
4. **KAN with Pruning**: Pruned KAN models for efficiency

## Notes

- MSE values are on a log scale in learning curves for better visualization
- Relative error in heatmaps is capped at 100% for visualization
- Cross-sections are taken at x = 0.25, 0.5, and 0.75
- Error quantiles divide errors into 5 regions: Q1 (0-25%), Q2 (25-50%), Q3 (50-75%), Q4 (75-90%), Q5 (>90%)

## Next Steps

Based on this analysis, you can:

1. Identify which model type performs best for your application
2. Determine if more training epochs are needed (check learning curves)
3. Find problematic regions in the domain that need special attention
4. Balance accuracy vs computational cost using the timing data
5. Make informed decisions about hyperparameter tuning

---

*This report was automatically generated by the Section 1 Analysis Pipeline.*
