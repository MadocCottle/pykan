# Analysis Summary Report

**Generated:** 2025-10-22 15:25:03

## Input Data

- **Results File:** `section1_1_20251022_144828.pkl`
- **Section Type:** section1_1
- **Dimensionality:** 1D
- **Models Directory:** /Users/main/Desktop/my_pykan/pykan/madoc/section1/results/sec1_results

## Generated Outputs

### 1. Pareto Frontier Analysis (NEW)

Location: `01_pareto_analysis/`

This directory contains:
- **Pareto frontier plots** - Log-log plots of test RMSE vs. parameter count
- **Best models tables** - CSV files with Pareto-optimal models per dataset
- **Scaling law plots** - Power-law fits showing architecture efficiency
- **Scaling summaries** - α exponent comparisons across architectures

**Key insights:**
- Pareto-optimal models minimize both error AND parameter count
- Higher α = architecture scales better with added parameters
- Direct comparison to Liu et al. (2024) KAN paper methodology

Files generated:
- `pareto_frontier_<N>.png` - Pareto frontier visualization
- `best_models_<N>.csv` - Pareto-optimal model list
- `scaling_laws_<N>.png` - Scaling law curves with α values
- `scaling_summary_<N>.csv` - Architecture scaling comparison

### 2. Function Fitting Visualizations

Location: `02_function_fitting/`

This directory contains visualizations comparing neural network predictions with true functions:

For 1D functions:
- **Line plots** showing true function vs NN prediction
- **Residual plots** showing prediction errors
- **MSE calculations** displayed on each plot
- **Parameter counts** in subplot titles (NEW)

Each visualization shows all model types (MLP, SIREN, KAN, KAN with pruning) side-by-side.


## How to Interpret Results

### Pareto Frontier Plots
- **X-axis**: Parameter count (log scale)
- **Y-axis**: Test RMSE (log scale)
- **Pareto-optimal models**: Highlighted with bold markers and black edges
- **Best architecture**: Lower-left = better (fewer parameters, lower error)

### Scaling Laws
- **Formula**: RMSE = A × N^(-α) where N = parameter count
- **α exponent**: Higher = better scaling efficiency
- **Interpretation**: If KAN has α=0.5 and MLP has α=0.3, KAN gains more accuracy per parameter added

### Best Models Table
- Only Pareto-optimal models (or top 3 per architecture if none)
- Sorted by test RMSE (ascending)
- Use this to identify: "What's the best model per architecture?"

## Methodology Notes

This analysis follows the comparison protocol from:
**Liu, Z., et al. (2024). KAN: Kolmogorov-Arnold Networks. arXiv:2404.19756**

Key principles:
1. Fair comparisons via parameter-aware metrics
2. Pareto optimality over exhaustive sweeps
3. Scaling laws reveal fundamental architecture efficiency
4. Focus on interpretability and actionable insights

---

*Analysis generated by Section 1 Analysis Pipeline v2.0*
