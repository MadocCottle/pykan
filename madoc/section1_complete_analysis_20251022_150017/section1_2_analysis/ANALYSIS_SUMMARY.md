# Analysis Summary Report

**Generated:** 2025-10-22 15:00:43

## Input Data

- **Results File:** `section1_2_20251022_143537.pkl`
- **Section Type:** section1_2
- **Dimensionality:** 1D
- **Models Directory:** /Users/main/Desktop/my_pykan/pykan/madoc/section1/results/sec2_results

## Generated Outputs

### 1. Comparative Metrics Analysis

Location: `01_comparative_metrics/`

This directory contains:
- **Comparison tables** (CSV) for each dataset showing final MSE, training times, etc.
- **Learning curves** showing train/test/dense MSE over epochs
- **Training time comparisons** across all model types
- **Performance heatmaps** showing final MSE across all datasets and models

Files generated:
- `dataset_<N>_comparison_table.csv` - Detailed metrics for dataset N
- `dataset_<N>_learning_curves_<metric>.png` - Learning curves for each metric
- `dataset_<N>_training_times.png` - Training time comparisons
- `all_datasets_heatmap_<metric>.png` - Overall performance heatmaps

### 2. Function Fitting Visualizations

Location: `02_function_fitting/`

This directory contains visualizations comparing neural network predictions with true functions:


For 1D functions:
- **Line plots** showing true function vs NN output
- **Point-by-point comparisons** across the domain
- **MSE calculations** for each model

Each visualization compares all model types (MLP, SIREN, KAN, KAN with pruning).




## How to Use These Results

### Quick Start

1. **Start with the heatmaps** in `01_comparative_metrics/all_datasets_heatmap_test.png` to see overall model performance
2. **Review learning curves** to understand training dynamics
3. **Examine function fits** to see how well models approximate each function
4. **Check training times** if computational efficiency is important

### Detailed Analysis

For each dataset/function of interest:

1. Open the **comparison table CSV** to see exact numerical values
2. Look at **learning curves** to check for overfitting or convergence issues
3. Examine **function fitting plots** to visually assess approximation quality


### Key Metrics

- **Train MSE**: Error on training data (lower is better)
- **Test MSE**: Error on test data (lower is better, indicates generalization)
- **Dense MSE**: Dense sampling MSE (better indicator of true approximation quality)
- **Total Time**: Complete training time in seconds
- **Time per Epoch**: Average time per training epoch

## Model Comparison

The analysis compares four model types:

1. **MLP**: Traditional multilayer perceptron with various depths and activations
2. **SIREN**: Sinusoidal activation networks, specialized for periodic functions
3. **KAN**: Kolmogorov-Arnold Networks with various grid sizes
4. **KAN with Pruning**: Pruned KAN models for efficiency

## Notes

- MSE values are on a log scale in learning curves for better visualization
- Relative error in heatmaps is capped at 100% for visualization
- Cross-sections are taken at x = 0.25, 0.5, and 0.75
- Error quantiles divide errors into 5 regions: Q1 (0-25%), Q2 (25-50%), Q3 (50-75%), Q4 (75-90%), Q5 (>90%)

## Next Steps

Based on this analysis, you can:

1. Identify which model type performs best for your application
2. Determine if more training epochs are needed (check learning curves)
3. Find problematic regions in the domain that need special attention
4. Balance accuracy vs computational cost using the timing data
5. Make informed decisions about hyperparameter tuning

---

*This report was automatically generated by the Section 1 Analysis Pipeline.*
